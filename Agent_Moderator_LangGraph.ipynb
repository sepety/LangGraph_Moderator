{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Install Dependencies & Basic Imports"
      ],
      "metadata": {
        "id": "Bi5RHzZmAEDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we install the necessary packages and import required modules.\n",
        "We install langgraph, langchain-openai, pinecone-client, and the CLIP model from GitHub,\n",
        "as well as opencv-python and torch."
      ],
      "metadata": {
        "id": "z6K8AQ7qASq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langgraph langchain-openai pinecone-client\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install opencv-python torch\n",
        "\n",
        "import os\n",
        "os.environ[\"USER_AGENT\"] = \"your_user_agent\"  # Set your user agent\n",
        "\n",
        "# Imports for text moderation (using ChatOpenAI)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Imports for image moderation (using CLIP)\n",
        "import torch, clip\n",
        "from PIL import Image\n",
        "\n",
        "# Imports for agent creation (using langgraph)\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.schema import AIMessage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WlOH8LCAsj_",
        "outputId": "2681e348-e4b9-4bda-fe50-6a0eb77dd9bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.45)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.21)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.3)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.58)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.66.3 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.66.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.10.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yh1cgl1i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-yh1cgl1i\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Initialize Models for Moderation\n",
        "\n",
        " Here we initialize two models:\n",
        " 1. A ChatOpenAI model for text moderation (e.g., GPT‑4).\n",
        "2. A CLIP model for image moderation.\n",
        " Replace \"YOUR_OPENAI_API_KEY\" with your actual OpenAI API key."
      ],
      "metadata": {
        "id": "_8QEbQcwA1WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
        "model_text = ChatOpenAI(\n",
        "    model_name=\"gpt-4\",\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# Initialize the CLIP model (using ViT-B/32) for image processing.\n",
        "# The model will use CUDA if available, otherwise CPU.\n",
        "clip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=clip_device)"
      ],
      "metadata": {
        "id": "pCNoRDphBBqD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Define Moderation Functions\n",
        "\n",
        " In this cell we define two key functions:\n",
        " 1. moderate_review_tool: Analyzes a review text using GPT-4 to check for offensive language,\n",
        "    aggression, insults, and calls for violence. It returns a rejection message if any issues are found.\n",
        " 2. moderate_image_tool: Loads an image and computes its embedding with CLIP, then compares it to a set\n",
        "    of predefined descriptions of inappropriate content. If the similarity for any description exceeds\n",
        "    a threshold, the image is flagged."
      ],
      "metadata": {
        "id": "xQEvsMPcBIid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate_review_tool(review: str) -> str:\n",
        "    \"\"\"\n",
        "    Moderates text reviews.\n",
        "    Analyzes the input text for offensive language, aggression, insults, and calls for violence.\n",
        "    Returns a rejection message with an explanation if any issues are detected,\n",
        "    otherwise confirms that the review passed moderation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "You are a content moderator. Analyze the following text for the presence of:\n",
        "- Offensive language,\n",
        "- Aggression or insults,\n",
        "- Implicit metaphorical insults,\n",
        "- Calls for violence.\n",
        "\n",
        "If you find any of these elements, politely refuse to publish the review, explain what was found,\n",
        "and suggest a more appropriate version. If the text is clean, answer: \"The review passed moderation and can be published.\"\n",
        "\n",
        "Review:\n",
        "{review}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "        response = model_text.invoke([HumanMessage(content=prompt)])\n",
        "        return response.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error moderating text: {str(e)}\"\n",
        "\n",
        "def moderate_image_tool(image_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Moderates images.\n",
        "    Loads an image, computes its embedding using CLIP, and compares it against a set of predefined\n",
        "    descriptions of inappropriate content.\n",
        "    If the similarity with any description exceeds the threshold, the image is flagged as inappropriate.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_input = clip_preprocess(image).unsqueeze(0).to(clip_device)\n",
        "        # Predefined descriptions of inappropriate content\n",
        "        prompts = [\n",
        "            \"pornographic content\",\n",
        "            \"explicit sexual content\",\n",
        "            \"extremist propaganda\",\n",
        "            \"fascist symbols\",\n",
        "            \"graphic violence\",\n",
        "            \"topless woman\",\n",
        "            \"nudity\",\n",
        "            \"bare breasts\",\n",
        "            \"exposed breasts\"\n",
        "        ]\n",
        "        text_inputs = torch.cat([clip.tokenize(prompt) for prompt in prompts]).to(clip_device)\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            text_features = clip_model.encode_text(text_inputs)\n",
        "        # Normalize features to compute cosine similarity\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        similarities = (image_features @ text_features.T).squeeze(0)\n",
        "        threshold = 0.25  # Adjust this threshold as needed\n",
        "        flagged_prompts = [prompts[i] for i, sim in enumerate(similarities) if sim > threshold]\n",
        "        if flagged_prompts:\n",
        "            return (f\"Inappropriate content detected: {', '.join(flagged_prompts)}. \"\n",
        "                    \"The image cannot be published.\")\n",
        "        else:\n",
        "            return \"The image passed moderation and can be published.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error moderating image: {str(e)}\""
      ],
      "metadata": {
        "id": "1Hrt4wqvBOzd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: Wrap Functions into Structured Tools\n",
        "==============================================================================\n",
        " In this cell, we define Pydantic schemas for the input data and wrap our moderation functions\n",
        " as StructuredTool objects. This allows the agent to call them based on commands."
      ],
      "metadata": {
        "id": "AXq1w_S9BUj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.tools import StructuredTool\n",
        "\n",
        "class ModerateReviewInput(BaseModel):\n",
        "    review: str = Field(..., description=\"The review text to be moderated.\")\n",
        "\n",
        "class ModerateImageInput(BaseModel):\n",
        "    image_path: str = Field(..., description=\"The file path to the image to be moderated.\")\n",
        "\n",
        "moderate_review_tool_structured = StructuredTool.from_function(\n",
        "    func=moderate_review_tool,\n",
        "    name=\"moderate_review_tool\",\n",
        "    description=(\n",
        "        \"Moderates text reviews by checking for offensive language, aggression, and calls for violence. \"\n",
        "        \"Returns a rejection with an explanation if issues are found.\"\n",
        "    ),\n",
        "    args_schema=ModerateReviewInput\n",
        ")\n",
        "\n",
        "moderate_image_tool_structured = StructuredTool.from_function(\n",
        "    func=moderate_image_tool,\n",
        "    name=\"moderate_image_tool\",\n",
        "    description=(\n",
        "        \"Moderates images by comparing them with predefined descriptions of inappropriate content (e.g., pornography, extremism, fascist symbols, violence). \"\n",
        "        \"Returns a message if inappropriate content is detected.\"\n",
        "    ),\n",
        "    args_schema=ModerateImageInput\n",
        ")"
      ],
      "metadata": {
        "id": "14NE-OVfBhbw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Create the Moderation Agent\n",
        " ==============================================================================\n",
        " Now we create an agent that uses only our two moderation tools.\n",
        " We initialize a MemorySaver to keep track of the conversation history and use a StateGraph to manage dialogue state.\n"
      ],
      "metadata": {
        "id": "g48HYNcvBsWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    moderate_review_tool_structured,\n",
        "    moderate_image_tool_structured\n",
        "]\n",
        "\n",
        "# Initialize agent memory for conversation tracking.\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Create the agent with a system prompt focused on content moderation.\n",
        "agent_executor = create_react_agent(\n",
        "    model=model_text,  # We use the text model for processing commands\n",
        "    tools=tools,\n",
        "    checkpointer=memory,\n",
        "    state_modifier=(\n",
        "        \"You are a content moderator. Your task is to check text reviews and images for inappropriate content. \"\n",
        "        \"Use the provided moderation tools and return an appropriate message based on your analysis.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Define the conversation state schema.\n",
        "from typing import Sequence, TypedDict\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence, add_messages]\n",
        "\n",
        "def call_agent(state: State):\n",
        "    response = agent_executor.invoke({\"messages\": state[\"messages\"]})\n",
        "    return {\"messages\": state[\"messages\"] + response[\"messages\"]}\n",
        "\n",
        "# Create the state graph and compile the agent.\n",
        "workflow = StateGraph(state_schema=State)\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_node(\"agent\", call_agent)\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "0Cyr8jUVBxQY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6: Interactive Chat for Testing the Moderation Agent\n",
        " ==============================================================================\n",
        " This cell launches an interactive console chat.\n",
        " You can input commands like:\n",
        "   \"Use moderate_image_tool {\\\"image_path\\\": \\\"/path/to/image.jpg\\\"}\"\n",
        " or any text review that the agent will moderate."
      ],
      "metadata": {
        "id": "oin9WyN2B6D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_chat():\n",
        "    print(\"Content Moderation Agent is running. Type 'exit' to quit.\")\n",
        "    state = {\"messages\": []}\n",
        "    config = {\"configurable\": {\"thread_id\": \"moderation_session_1\"}}\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Session terminated.\")\n",
        "            break\n",
        "        # Create a human message and add it to the conversation state.\n",
        "        from langchain_core.messages import HumanMessage\n",
        "        human_message = HumanMessage(content=user_input)\n",
        "        state[\"messages\"].append(human_message)\n",
        "        try:\n",
        "            result = app.invoke({\"messages\": state[\"messages\"]}, config=config)\n",
        "            ai_response = result[\"messages\"][-1]\n",
        "            state[\"messages\"].append(ai_response)\n",
        "            print(f\"Agent: {ai_response.content}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling the agent: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt_XumjICAjN",
        "outputId": "6e3a9b4f-6f29-4474-cb2a-c93f8b51d460"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content Moderation Agent is running. Type 'exit' to quit.\n",
            "You: /content/drive/MyDrive/Guapos/640.gallery4_8761197d325541f926b015130fbb6bd0.jpg\n",
            "Agent: The image you've submitted contains inappropriate content, specifically nudity. As per our community guidelines, we cannot publish this image. Please submit a different image that adheres to our content policies.\n",
            "You: exit\n",
            "Session terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Notes\n",
        "Concept Overview:\n",
        "This notebook is designed as a \"recipe\" for building a content moderation agent. It demonstrates how to use GPT‑4 via ChatOpenAI to moderate text reviews and CLIP to moderate images.\n",
        "\n",
        "Agent Structure:\n",
        "The agent uses LangGraph's MemorySaver and StateGraph to manage dialogue state, making it easy to integrate into an interactive session.\n",
        "\n",
        "Usage:\n",
        "To test image moderation, input a command in the chat window such as:\n",
        "\n"
      ],
      "metadata": {
        "id": "lTjFKHi0Dg1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use moderate_image_tool {\"image_path\": \"/path/to/your/image.jpg\"}\n"
      ],
      "metadata": {
        "id": "4vXJdfMLDpGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For text moderation, simply type a review message."
      ],
      "metadata": {
        "id": "0aj1_gdYD1Nl"
      }
    }
  ]
}